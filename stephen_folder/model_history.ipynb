{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import prepare as p\n",
    "import explore as e\n",
    "\n",
    "from scipy import stats\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# modeling methods\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('school_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p.clean_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1391, 16)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1391 entries, 0 to 1560\n",
      "Data columns (total 16 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   school_id                     1391 non-null   int64  \n",
      " 1   english_1                     1391 non-null   float64\n",
      " 2   english_2                     1391 non-null   float64\n",
      " 3   algebra                       1391 non-null   float64\n",
      " 4   biology                       1391 non-null   float64\n",
      " 5   history                       1391 non-null   float64\n",
      " 6   bilingual_or_english_learner  1391 non-null   float64\n",
      " 7   econdis                       1391 non-null   float64\n",
      " 8   salary                        1391 non-null   float64\n",
      " 9   teacher_exp_6to10             1391 non-null   float64\n",
      " 10  extracurricular_expend        1391 non-null   float64\n",
      " 11  total_expend                  1391 non-null   float64\n",
      " 12  student_teacher_ratio         1391 non-null   float64\n",
      " 13  teacher_exp_0to5              1391 non-null   float64\n",
      " 14  teacher_exp_11_plus           1391 non-null   float64\n",
      " 15  high_edu                      1391 non-null   float64\n",
      "dtypes: float64(15), int64(1)\n",
      "memory usage: 184.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split\n",
    "train, val, test  = e.tts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((778, 16), (334, 16), (279, 16))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 778 entries, 404 to 1056\n",
      "Data columns (total 16 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   school_id                     778 non-null    int64  \n",
      " 1   english_1                     778 non-null    float64\n",
      " 2   english_2                     778 non-null    float64\n",
      " 3   algebra                       778 non-null    float64\n",
      " 4   biology                       778 non-null    float64\n",
      " 5   history                       778 non-null    float64\n",
      " 6   bilingual_or_english_learner  778 non-null    float64\n",
      " 7   econdis                       778 non-null    float64\n",
      " 8   salary                        778 non-null    float64\n",
      " 9   teacher_exp_6to10             778 non-null    float64\n",
      " 10  extracurricular_expend        778 non-null    float64\n",
      " 11  total_expend                  778 non-null    float64\n",
      " 12  student_teacher_ratio         778 non-null    float64\n",
      " 13  teacher_exp_0to5              778 non-null    float64\n",
      " 14  teacher_exp_11_plus           778 non-null    float64\n",
      " 15  high_edu                      778 non-null    float64\n",
      "dtypes: float64(15), int64(1)\n",
      "memory usage: 103.3 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate target variable\n",
    "# scale\n",
    "def iso_target_variable(df):\n",
    "    X = df.drop(columns = ['english_1', 'english_2', 'algebra', 'biology', 'history'])\n",
    "    y = df['history']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root Mean Squared Error RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, y_Train = iso_target_variable(train)\n",
    "X_Val, y_Val = iso_target_variable(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_id</th>\n",
       "      <th>english_1</th>\n",
       "      <th>english_2</th>\n",
       "      <th>algebra</th>\n",
       "      <th>biology</th>\n",
       "      <th>history</th>\n",
       "      <th>bilingual_or_english_learner</th>\n",
       "      <th>econdis</th>\n",
       "      <th>salary</th>\n",
       "      <th>teacher_exp_6to10</th>\n",
       "      <th>extracurricular_expend</th>\n",
       "      <th>total_expend</th>\n",
       "      <th>student_teacher_ratio</th>\n",
       "      <th>teacher_exp_0to5</th>\n",
       "      <th>teacher_exp_11_plus</th>\n",
       "      <th>high_edu</th>\n",
       "      <th>yhat_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>57905088</td>\n",
       "      <td>98.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>87.9</td>\n",
       "      <td>69483.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7558.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>50.7</td>\n",
       "      <td>44.8</td>\n",
       "      <td>87.829049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>57912005</td>\n",
       "      <td>45.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>47.5</td>\n",
       "      <td>85.4</td>\n",
       "      <td>63990.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24531.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>51.9</td>\n",
       "      <td>27.9</td>\n",
       "      <td>31.1</td>\n",
       "      <td>87.829049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>81902002</td>\n",
       "      <td>69.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>53.5</td>\n",
       "      <td>56116.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>9636.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>29.7</td>\n",
       "      <td>65.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>87.829049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     school_id  english_1  english_2  algebra  biology  history  \\\n",
       "404   57905088       98.0      100.0    100.0    100.0    100.0   \n",
       "429   57912005       45.0       47.0     79.0     94.0     75.0   \n",
       "621   81902002       69.0       79.0     74.0     86.0     93.0   \n",
       "\n",
       "     bilingual_or_english_learner  econdis   salary  teacher_exp_6to10  \\\n",
       "404                          30.5     87.9  69483.0               26.9   \n",
       "429                          47.5     85.4  63990.0               20.2   \n",
       "621                           4.5     53.5  56116.0                5.2   \n",
       "\n",
       "     extracurricular_expend  total_expend  student_teacher_ratio  \\\n",
       "404                    38.0        7558.0                   19.2   \n",
       "429                    13.0       24531.0                    5.9   \n",
       "621                  1071.0        9636.0                   11.3   \n",
       "\n",
       "     teacher_exp_0to5  teacher_exp_11_plus  high_edu  yhat_baseline  \n",
       "404              22.4                 50.7      44.8      87.829049  \n",
       "429              51.9                 27.9      31.1      87.829049  \n",
       "621              29.7                 65.2      22.8      87.829049  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline predictions\n",
    "\n",
    "train['yhat_baseline'] = y_Train.mean()\n",
    "train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.018928764909456"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mse_train = mean_squared_error(y_Train, train.yhat_baseline)\n",
    "rmse_train= np.sqrt(mse_train)###############################################\n",
    "rmse_train\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline RMSE = 11.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GLM(power, alpha, X_Train, y_Train, X_Val, y_Val):\n",
    "    \n",
    "\n",
    "    scaler = pre.MinMaxScaler()\n",
    "    # Note that we only call .fit with the training data,\n",
    "    # but we use .transform to apply the scaling to all the data splits.\n",
    "    scaler.fit(X_Train)\n",
    "\n",
    "    x_train_scaled = scaler.transform(X_Train)\n",
    "    x_validate_scaled = scaler.transform(X_Val)\n",
    "    #x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    # create the model object\n",
    "    glm = TweedieRegressor(power=power, alpha=alpha)\n",
    "\n",
    "    # fit the model to training data. must specify the column in y_train\n",
    "    glm.fit(x_train_scaled, y_Train)\n",
    "\n",
    "    # predict train\n",
    "    y_pred_train = glm.predict(x_train_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    mse_train = mean_squared_error(y_Train, y_pred_train)\n",
    "    rmse_train= np.sqrt(mse_train)###############################################\n",
    "\n",
    "    # predict validate\n",
    "    glm.fit(x_validate_scaled, y_Val)\n",
    "    y_pred_val = glm.predict(x_validate_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    mse_val = mean_squared_error(y_Val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mse_val)##################################################\n",
    "\n",
    "\n",
    "    \n",
    "    return print(\"RMSE for GLM using Tweedie Regressor \\nTraining/In-Sample: \", round(rmse_train), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_Train.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for GLM using Tweedie Regressor \n",
      "Training/In-Sample:  9 \n",
      "Validation/Out-of-Sample:  8\n"
     ]
    }
   ],
   "source": [
    "#power =1 for poisson distribution of target variable\n",
    "GLM(1, 0, X_Train, y_Train, X_Val, y_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Andy's function to see if polynomial regression is better than Tweedie Regressor\n",
    "def subject_model(df, target, degree=2):\n",
    "    '''\n",
    "    this function will split, scale, and model english 1\n",
    "    '''\n",
    "    train, val, test=e.tts(df)\n",
    "    \n",
    "    X_train=train.drop(columns=['biology', 'english_1', 'english_2', 'algebra', 'history', 'school_id'])\n",
    "    y_train=pd.DataFrame(train[target])\n",
    "\n",
    "    X_val=val.drop(columns=['biology', 'english_1', 'english_2', 'algebra', 'history', 'school_id'])\n",
    "    y_val=pd.DataFrame(val[target])\n",
    "\n",
    "    X_test=test.drop(columns=['biology', 'english_1', 'english_2', 'algebra', 'history', 'school_id'])\n",
    "    y_test=pd.DataFrame(test[target])\n",
    "    \n",
    "    scaler=pre.MinMaxScaler()\n",
    "    \n",
    "    num_cols = ['bilingual_or_english_learner',\n",
    "            'econdis',\n",
    "            'salary',\n",
    "            'teacher_exp_6to10',\n",
    "            'extracurricular_expend',\n",
    "            'total_expend',\n",
    "            'student_teacher_ratio',\n",
    "            'teacher_exp_0to5',\n",
    "            'teacher_exp_11_plus',\n",
    "            'high_edu']\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "    \n",
    "    '''\n",
    "    this model will create the object and run the polynomyal regression model for english 1\n",
    "    '''\n",
    "    pf = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    X_train_degree2 = pf.fit_transform(X_train)\n",
    "    X_validate_degree2 = pf.transform(X_val)\n",
    "    X_test_degree2 = pf.transform(X_test)\n",
    "    \n",
    "    lm2 = LinearRegression()\n",
    "    lm2.fit(X_train_degree2, y_train[target])\n",
    "\n",
    "    y_train[f'{target}_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "    rmse_train = mean_squared_error(y_train[target], y_train[f'{target}_pred_lm2'], squared=False)\n",
    "\n",
    "    y_val[f'{target}_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "    rmse_validate = mean_squared_error(y_val[target], y_val[f'{target}_pred_lm2'], squared=False)\n",
    "\n",
    "    print(f\"RMSE for Polynomial Model, degrees={degree}\\nTraining/In-Sample: \", rmse_train, \n",
    "          \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Polynomial Model, degrees=1\n",
      "Training/In-Sample:  9.387695076749957 \n",
      "Validation/Out-of-Sample:  8.717283608982898\n"
     ]
    }
   ],
   "source": [
    "subject_model(df, 'history', degree= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try StandardScaler and RobustScaler on LassoLars\n",
    "\n",
    "\n",
    "#using Andy's function to see if polynomial regression is better than Tweedie Regressor\n",
    "def subject_model(df, target):\n",
    "    '''\n",
    "    this function will split, scale, and model english 1\n",
    "    '''\n",
    "    train, val, test=e.tts(df)\n",
    "    \n",
    "    X_train=train.drop(columns=['biology', 'english_1', 'english_2', 'algebra', 'history', 'school_id'])\n",
    "    y_train=pd.DataFrame(train[target])\n",
    "\n",
    "    X_val=val.drop(columns=['biology', 'english_1', 'english_2', 'algebra', 'history', 'school_id'])\n",
    "    y_val=pd.DataFrame(val[target])\n",
    "\n",
    "    X_test=test.drop(columns=['biology', 'english_1', 'english_2', 'algebra', 'history', 'school_id'])\n",
    "    y_test=pd.DataFrame(test[target])\n",
    "    \n",
    "    scaler=pre.StandardScaler()\n",
    "    \n",
    "    num_cols = ['bilingual_or_english_learner',\n",
    "            'econdis',\n",
    "            'salary',\n",
    "            'teacher_exp_6to10',\n",
    "            'extracurricular_expend',\n",
    "            'total_expend',\n",
    "            'student_teacher_ratio',\n",
    "            'teacher_exp_0to5',\n",
    "            'teacher_exp_11_plus',\n",
    "            'high_edu']\n",
    "    \n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train[num_cols] = scaler.transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "    \n",
    "    '''\n",
    "    this model will create the object and run the polynomyal regression model for english 1\n",
    "    '''\n",
    "    lars = LassoLars(alpha=1.0)\n",
    "\n",
    "\n",
    "    X_train_degree2 = lars.fit(X_train, y_train)\n",
    "    #X_validate_degree2 = lars.transform(X_val)\n",
    "    #X_test_degree2 = lars.transform(X_test)\n",
    "\n",
    "    x_train_pred\n",
    "\n",
    "    y_train[f'{target}_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "    rmse_train = mean_squared_error(y_train[target], y_train[f'{target}_pred_lm2'], squared=False)\n",
    "\n",
    "    y_val[f'{target}_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "    rmse_validate = mean_squared_error(y_val[target], y_val[f'{target}_pred_lm2'], squared=False)\n",
    "\n",
    "    print(f\"RMSE for Polynomial Model, degrees={degree}\\nTraining/In-Sample: \", rmse_train, \n",
    "          \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=LassoLars().\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m subject_model(df, \u001b[39m'\u001b[39;49m\u001b[39mhistory\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb Cell 23\u001b[0m in \u001b[0;36msubject_model\u001b[0;34m(df, target)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb#X62sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#X_validate_degree2 = lars.transform(X_val)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb#X62sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#X_test_degree2 = lars.transform(X_test)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb#X62sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m lm2 \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb#X62sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m lm2\u001b[39m.\u001b[39;49mfit(X_train_degree2, y_train[target])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb#X62sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m y_train[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m_pred_lm2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m lm2\u001b[39m.\u001b[39mpredict(X_train_degree2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yogibexar/codeup-data-science/capstone_readiness/stephen_folder/model_history.ipynb#X62sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m rmse_train \u001b[39m=\u001b[39m mean_squared_error(y_train[target], y_train[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m_pred_lm2\u001b[39m\u001b[39m'\u001b[39m], squared\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:684\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    680\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    682\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 684\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    685\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    688\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[1;32m    689\u001b[0m     sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    690\u001b[0m )\n\u001b[1;32m    692\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[1;32m    693\u001b[0m     X,\n\u001b[1;32m    694\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[1;32m    699\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:871\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39mif\u001b[39;00m ensure_2d:\n\u001b[1;32m    869\u001b[0m     \u001b[39m# If input is scalar raise error\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 871\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    872\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got scalar array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    875\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    876\u001b[0m         )\n\u001b[1;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=LassoLars().\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "subject_model(df, 'history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def OLS(X_Train, y_Train, X_Val, y_Val):\n",
    "    \n",
    "\n",
    "    scaler = pre.StandardScaler()\n",
    "    # Note that we only call .fit with the training data,\n",
    "    # but we use .transform to apply the scaling to all the data splits.\n",
    "    X_Train_scaled = scaler.fit(X_Train)\n",
    "\n",
    "    # create the model object\n",
    "    \n",
    "\n",
    "    # fit the model to training data. must specify the column in y_train\n",
    "    glm.fit(x_train_scaled, y_Train)\n",
    "\n",
    "    # predict train\n",
    "    y_pred_train = glm.predict(x_train_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    mse_train = mean_squared_error(y_Train, y_pred_train)\n",
    "    rmse_train= np.sqrt(mse_train)###############################################\n",
    "\n",
    "    # predict validate\n",
    "    glm.fit(x_validate_scaled, y_Val)\n",
    "    y_pred_val = glm.predict(x_validate_scaled)\n",
    "\n",
    "    # evaluate: rmse\n",
    "    mse_val = mean_squared_error(y_Val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mse_val)##################################################\n",
    "\n",
    "\n",
    "    \n",
    "    return print(\"RMSE for GLM using Tweedie Regressor \\nTraining/In-Sample: \", round(rmse_train), \n",
    "      \"\\nValidation/Out-of-Sample: \", round(rmse_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
